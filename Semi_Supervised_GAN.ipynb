{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semi-Supervised GAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMW44y/QTbrEjxojhnE56JG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akiyoshi-Yagi/GANs/blob/master/Semi_Supervised_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_lMtS7kAus3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "500e7bd7-4183-4e52-adfa-24a9e70d4246"
      },
      "source": [
        "#諸々import\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Activation, BatchNormalization, Concatenate, Dense, Dropout, Flatten, Input, Lambda, Reshape\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix-6eOADCRT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#モデルの入力次元の設定\n",
        "\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "z_dim = 100\n",
        "num_classes = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUL9COoNCvjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#訓練とテストの為のデータの整形\n",
        "\n",
        "class Dataset:\n",
        "  def __init__(self, num_labeled):\n",
        "    self.num_labeled = num_labeled\n",
        "    (self.x_train, self.y_train),(self.x_test, self.y_test) = mnist.load_data()\n",
        "\n",
        "    def preprocess_imgs(x):\n",
        "      x = (x.astype(np.float32) - 127.5) /127.5\n",
        "      #x = (x / 127.5) -1.0\n",
        "      x = np.expand_dims(x, axis = 3)\n",
        "      return x\n",
        "    \n",
        "    def preprocess_labels(y):\n",
        "      return y.reshape(-1, 1)\n",
        "\n",
        "\n",
        "    self.x_train = preprocess_imgs(self.x_train)\n",
        "    self.y_train = preprocess_labels(self.y_train)\n",
        "\n",
        "    self.x_test = preprocess_imgs(self.x_test)\n",
        "    self.y_test = preprocess_labels(self.y_test)\n",
        "  \n",
        "  def batch_labeled(self, batch_size):\n",
        "    idx = np.random.randint(0, self.num_labeled, batch_size)\n",
        "    imgs = self.x_train[idx]\n",
        "    labels = self.y_train[idx]\n",
        "    return imgs, labels\n",
        "  \n",
        "  def batch_unlabeled(self, batch_size):\n",
        "    idx = np.random.randint(self.num_labeled, self.x_train.shape[0], batch_size)\n",
        "    imgs = self.x_train[idx]\n",
        "\n",
        "    return imgs\n",
        "  \n",
        "  def training_set(self):\n",
        "    x_train = self.x_train[range(self.num_labeled)]\n",
        "    y_train = self.y_train[range(self.num_labeled)]\n",
        "    return x_train, y_train\n",
        "  \n",
        "  def test_set(self):\n",
        "    return self.x_test, self.y_test\n",
        "  \n",
        "num_labeled = 100\n",
        "dataset = Dataset(num_labeled)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evca_AtTGnuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#生成器\n",
        "\n",
        "def build_generator(z_dim):\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256*7*7, input_dim=z_dim))\n",
        "  model.add(Reshape((7, 7, 256)))\n",
        "\n",
        "  model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "  model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"same\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "  model.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"same\"))\n",
        "  model.add(Activation(\"tanh\"))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk_vD00yG1ro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#識別器\n",
        "\n",
        "def build_discriminator_net(img_shape):\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "  model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "  model.add(Conv2D(128, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(num_classes))\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_discriminator_supervised(discriminator_net):\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(discriminator_net)\n",
        "  model.add(Activation(\"softmax\"))\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_discriminator_unsupervised(discriminator_net):\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(discriminator_net)\n",
        "  def predict(x):\n",
        "    '''\n",
        "    10ニューロンの出力大きい（つまりニューロンが入力画像を本物だと知覚した)時、predictionは1.0に近づく。\n",
        "    逆に、10ニューロンの出力が全て小さい（つまりニューロンが入力画像を偽物だと知覚した）時、predictionは0に近づく。\n",
        "    '''\n",
        "    prediction = 1.0 - (1.0 /(K.sum(K.exp(x), axis=-1, keepdims=True) + 1.0))\n",
        "    return prediction\n",
        "  model.add(Lambda(predict))\n",
        "\n",
        "  return model  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIeqboH4XlI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#モデルの構築\n",
        "def build_gan(generator, discriminator):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "\n",
        "    return model\n",
        "\n",
        "discriminator_net = build_discriminator_net(img_shape)\n",
        "\n",
        "# 教師あり学習のコンパイル\n",
        "discriminator_supervised = build_discriminator_supervised(discriminator_net)\n",
        "discriminator_supervised.compile(loss='categorical_crossentropy',\n",
        "                                 metrics=['accuracy'],\n",
        "                                 optimizer=Adam())\n",
        "\n",
        "# 教師なし学習のコンパイル\n",
        "discriminator_unsupervised = build_discriminator_unsupervised(discriminator_net)\n",
        "discriminator_unsupervised.compile(loss='binary_crossentropy',\n",
        "                                   optimizer=Adam())\n",
        "\n",
        "#生成器のコンパイル\n",
        "generator = build_generator(z_dim)\n",
        "discriminator_unsupervised.trainable = False\n",
        "\n",
        "gan = build_gan(generator, discriminator_unsupervised)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiQdlbt_ZWLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#訓練アルゴリズム\n",
        "\n",
        "supervised_losses = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "def train(iterations, batch_size, sample_interval):\n",
        "\n",
        "  real = np.ones((batch_size, 1))\n",
        "  fake = np.zeros((batch_size, 1))\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "\n",
        "    #ラベル付きサンプルの取得\n",
        "    imgs, labels = dataset.batch_labeled(batch_size)\n",
        "    labels = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "    #ラベルなしサンプルの取得\n",
        "    imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
        "\n",
        "    #偽画像バッチの作成\n",
        "    z = np.random.normal(0, 1, ((batch_size, z_dim)))\n",
        "    gen_imgs = generator.predict(z)\n",
        "\n",
        "    d_loss_supervised, accuracy = discriminator_supervised.train_on_batch(imgs, labels)\n",
        "    d_loss_real = discriminator_unsupervised.train_on_batch(imgs_unlabeled, real)\n",
        "\n",
        "    d_loss_fake = discriminator_unsupervised.train_on_batch(gen_imgs, fake)\n",
        "\n",
        "    d_loss_unsupervised = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    z = np.random.normal(0, 1, (batch_size, z_dim))\n",
        "    gen_imgs = generator.predict(z)\n",
        "\n",
        "    g_loss = gan.train_on_batch(z, real)\n",
        "\n",
        "    if (iteration + 1) % sample_interval == 0:\n",
        "      supervised_losses.append(d_loss_supervised)\n",
        "      iteration_checkpoints.append(iteration+1)\n",
        "      print(\n",
        "                \"%d [D loss supervised: %.4f, acc.: %.2f%%] [D loss unsupervised: %.4f] [G loss: %f]\"\n",
        "                % (iteration + 1, d_loss_supervised, 100 * accuracy,\n",
        "                   d_loss_unsupervised, g_loss))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMk0vxGQyU5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "3f115873-0fb1-41cb-a19a-91c3a2fadc1d"
      },
      "source": [
        "iterations = 8000\n",
        "batch_size = 32\n",
        "sample_interval = 800\n",
        "\n",
        "train(iterations, batch_size, sample_interval)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "800 [D loss supervised: 0.0045, acc.: 100.00%] [D loss unsupervised: 0.0361] [G loss: 3.187121]\n",
            "1600 [D loss supervised: 0.0027, acc.: 100.00%] [D loss unsupervised: 0.2585] [G loss: 5.545338]\n",
            "2400 [D loss supervised: 0.0004, acc.: 100.00%] [D loss unsupervised: 0.3236] [G loss: 3.961680]\n",
            "3200 [D loss supervised: 0.0015, acc.: 100.00%] [D loss unsupervised: 0.2342] [G loss: 5.902021]\n",
            "4000 [D loss supervised: 0.0004, acc.: 100.00%] [D loss unsupervised: 0.1315] [G loss: 4.544368]\n",
            "4800 [D loss supervised: 0.0006, acc.: 100.00%] [D loss unsupervised: 0.4782] [G loss: 5.093963]\n",
            "5600 [D loss supervised: 0.0006, acc.: 100.00%] [D loss unsupervised: 0.3707] [G loss: 4.130798]\n",
            "6400 [D loss supervised: 0.0002, acc.: 100.00%] [D loss unsupervised: 0.6565] [G loss: 3.110205]\n",
            "7200 [D loss supervised: 0.0009, acc.: 100.00%] [D loss unsupervised: 0.3305] [G loss: 3.829001]\n",
            "8000 [D loss supervised: 0.0001, acc.: 100.00%] [D loss unsupervised: 0.2278] [G loss: 4.093932]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IM6cOfOzHq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "be60a3d1-7c3f-45d0-b15c-eba4a60ed3e7"
      },
      "source": [
        "x, y = dataset.test_set()\n",
        "y = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# Compute classification accuracy on the test set\n",
        "_, accuracy = discriminator_supervised.evaluate(x, y)\n",
        "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 138us/step\n",
            "Test Accuracy: 90.54%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MVIvtuG-9IW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "50621f7a-6516-497d-d44d-2d5ecf2c9eed"
      },
      "source": [
        "x, y = dataset.training_set()\n",
        "y = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# Compute classification accuracy on the training set\n",
        "_, accuracy = discriminator_supervised.evaluate(x, y)\n",
        "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 0s 483us/step\n",
            "Training Accuracy: 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J6n-6Kf-_dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}